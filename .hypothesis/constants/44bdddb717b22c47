# file: C:\Users\MSII\AppData\Roaming\Python\Python314\site-packages\peft\utils\other.py
# hypothesis_version: 6.148.3

[-100, ',', ', ', '.', '0', '0.29.0', '1', '4.53.1', 'CONFIG_NAME', 'HF_HUB_OFFLINE', 'PREFIX_TUNING', 'Params4bit', 'WEIGHTS_NAME', '[^.]+\\.base_layer\\.', '__base__', '_hf_hook', '_modules', '_no_split_modules', '_tied_weights_keys', 'adapter_names', 'aqlm', 'auto_trainable', 'base_layer', 'base_layer.', 'checkpoint_format', 'config', 'cpu', 'cuda', 'd_model', 'ds_numel', 'eetq', 'embed_tokens', 'encoder_hidden_size', 'exllama_config', 'f', 'false', 'get_base_model', 'get_decoder', 'get_input_embeddings', 'gptq', 'head_dim', 'hidden_size', 'hqq', 'hqq_quantized', 'is_loaded_in_4bit', 'is_loaded_in_8bit', 'meta', 'mlu', 'modules_to_save', 'modules_to_tie', 'mps', 'n', 'n_embd', 'n_head', 'n_layer', 'no', 'npu', 'num_attention_heads', 'num_heads', 'num_hidden_layers', 'num_key_value_heads', 'num_layers', 'off', 'on', 'quantization_config', 'quantization_method', 't', 'text_config', 'tie_word_embeddings', 'token_adapter', 'token_type_ids', 'torchao', 'training wrapper', 'true', 'tuner_layer_cls', 'use_exllama', 'use_reentrant', 'version', 'weight', 'xla', 'xpu', 'y', 'yes']