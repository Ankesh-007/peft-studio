# file: C:\Users\MSII\AppData\Roaming\Python\Python314\site-packages\torch\nn\attention\flex_attention.py
# hypothesis_version: 6.148.3

[1.0, 2.0, 100, 128, '\n)', ', ', '-inf', '...', 'AuxOutput', 'AuxRequest', 'BlockMask', 'FlexKernelOptions', 'OUTPUT_LOGSUMEXP', 'OUTPUT_MAX', 'PRESCALE_QK', 'ROWS_GUARANTEED_SAFE', 'WRITE_DQ', 'and_masks', 'cpu', 'create_block_mask', 'create_mask', 'cuda', 'eager', 'flex_attention', 'hpu', 'noop_mask', 'or_masks', 'xpu', '█', '░']